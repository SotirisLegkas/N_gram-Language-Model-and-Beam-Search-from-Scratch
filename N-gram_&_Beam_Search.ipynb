{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installs,Imports & Corpus"
      ],
      "metadata": {
        "id": "sx22GsTz3P2B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8Y4ZNXJ3lPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdd334a1-c274-4374-ada6-c45b706504c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5 MB 12.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 748 kB 46.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 110 kB 9.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 893 kB 17.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#Necessary installs\n",
        "!pip install -q -U nltk\n",
        "!pip install -q -U LM\n",
        "!pip install -q -U Levenshtein \n",
        " # Necessary imports\n",
        "import nltk\n",
        "import math\n",
        " #Tokenizer that divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences\n",
        "nltk.download('punkt',quiet=True)\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from pprint import pprint\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import brown\n",
        "from sklearn.model_selection import train_test_split\n",
        "from Levenshtein import distance as ld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM7UZXYcAa6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e90efdc-032e-4f2c-e3c7-4bdec46dee6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The chosen corpus is 23715 sentences long\n"
          ]
        }
      ],
      "source": [
        "nltk.download('brown',quiet=True) # downloading the specific corpus\n",
        "sents = brown.sents(categories = ['adventure','belles_lettres','editorial','fiction','news'])  # categories of brown corpus we selected(We didn't select the whole brown corpus because it needed over 1 hour to compile)\n",
        "print('The chosen corpus is',len(sents),'sentences long') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1"
      ],
      "metadata": {
        "id": "HyBgD0-c2eTg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp1VYSgPkk1O"
      },
      "outputs": [],
      "source": [
        "#Lowercasing our data\n",
        "sents = list(map(lambda x: list(map(lambda y: y.lower(),x)),sents))\n",
        "#Splitting our data to train,test\n",
        "x_train , x_test = train_test_split(sents,train_size=0.8,test_size=0.2,random_state=1)\n",
        "#Extra test set for a very specific purpose\n",
        "x_test2=x_test[0:(len(x_test)//100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP4ig9Qjfoq-"
      },
      "outputs": [],
      "source": [
        "#Flattening our x_train list so we can count tokens that appear < 10\n",
        "def flatten(t):\n",
        "    return [item for sublist in t for item in sublist]\n",
        "\n",
        "flat_list = flatten(x_train)\n",
        "count_flat_list = nltk.FreqDist(flat_list) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Duqa0spQ2da"
      },
      "outputs": [],
      "source": [
        "#Creating a list of words that appear < 10\n",
        "tokens_to_remove=[]\n",
        "for i in flat_list:\n",
        "  if count_flat_list[i]<10:\n",
        "    tokens_to_remove.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TxI6GQhWD0z"
      },
      "outputs": [],
      "source": [
        "#Replacing words that appear < 10 with special token \"unk\" / Takes approximately 10 minutes using Runtime>Change Runtime Type>GPU from colab's options !!\n",
        "#For every sentence\n",
        "for i in range(len(x_train)):\n",
        "  #For every word in each sentence\n",
        "  for j in range(len(x_train[i])):\n",
        "    #Replace with unk if TRUE\n",
        "    if x_train[i][j] in tokens_to_remove:\n",
        "      x_train[i][j]='unk'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsDYmyT-RSzq"
      },
      "outputs": [],
      "source": [
        "# Initializing the counters using code provided to us from the slides\n",
        "unigram_counter = Counter()\n",
        "bigram_counter = Counter()\n",
        "trigram_counter = Counter()\n",
        "\n",
        "for sent in x_train:\n",
        "\n",
        "    # Update the unigram counter\n",
        "    unigram_counter.update([(gram,) for gram in [\"<start>\"] + sent+ [\"<end>\"]]) # <end> special token is saved in unigrams for our knseser ney implemetation\n",
        "    \n",
        "    # Update the bigram counter\n",
        "    bigram_pad_sent = [\"<start>\"] + sent +  ['<end>']    \n",
        "    bigram_counter.update([(gram1, gram2) for gram1, gram2 in zip(bigram_pad_sent, bigram_pad_sent[1:])])\n",
        "\n",
        "    # Update the trigram counter\n",
        "    trigram_pad_sent = [\"<start>\"]*2 + sent +  ['<end>']*2\n",
        "    trigram_counter.update([(gram1, gram2, gram3) for gram1, gram2, gram3 in zip(trigram_pad_sent, trigram_pad_sent[1:], trigram_pad_sent[2:])]) \n",
        "\n",
        "\n",
        "# pprint(unigram_counter.most_common(5))   \n",
        "# pprint(bigram_counter.most_common(5))\n",
        "# pprint(trigram_counter.most_common(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram probability function (Calculating the log probability to better showcase the results instead of showing the probability values which are pretty small)\n",
        "def biprob(w1,w2,a = 1): \n",
        "  alpha = a\n",
        "  if type(w1) == str:\n",
        "    w1 = w1.lower()\n",
        "  if type(w2) == str:\n",
        "    w2 = w2.lower()\n",
        "  # Bigram prob + laplace smoothing\n",
        "  bigram_prob = (bigram_counter[(w1, w2)] + alpha) / (unigram_counter[(w1,)] + alpha*vocabsize)\n",
        "  #print(\"bigram_prob: {0:.3f} \".format(bigram_prob))\n",
        "\n",
        "  bigram_log_prob = math.log2(bigram_prob)\n",
        "  #print(\"bigram_log_prob: {0:.3f}\".format(bigram_log_prob) )  \n",
        "  return bigram_log_prob"
      ],
      "metadata": {
        "id": "eB-JWDon1mFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for trigram probability (Calculating the log probability/Same as above )\n",
        "def triprob(w1,w2,w3,a=1):\n",
        "  alpha=a\n",
        "  w1 = w1.lower()\n",
        "  w2 = w2.lower()\n",
        "  w3 = w3.lower()\n",
        "  trigram_prob = (trigram_counter[(w1, w2,w3)] + alpha) / (bigram_counter[(w1,w2)] + alpha*vocabsize)\n",
        "  trigram_log_prob = math.log2(trigram_prob)\n",
        "  return trigram_log_prob"
      ],
      "metadata": {
        "id": "UsKJH49n1myx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_14WSuYSbLD"
      },
      "outputs": [],
      "source": [
        "# Flattening the list to get all the tokens\n",
        "flat_list2 = flatten(x_train) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrukVHt0UZCq"
      },
      "outputs": [],
      "source": [
        "# Storing the unique tokens to form our vocabulary (from the train set)\n",
        "vocabsize= len(set(flat_list2)) \n",
        "vocabtrain = set(flat_list2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myUydJZDYF3N"
      },
      "outputs": [],
      "source": [
        "#We replace the words that are not in the vocabulary with the special token unk\n",
        "#For every sentence in the test dataset\n",
        "for sent in range (len(x_test)) :\n",
        "  #For every word in each sentence in the test dataset\n",
        "  for w in range ( len(x_test[sent])) :\n",
        "    #Replace with unk if TRUE\n",
        "    if x_test[sent][w] not in vocabtrain:\n",
        "      x_test[sent][w]='unk'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "\n"
      ],
      "metadata": {
        "id": "v9mqOt5Z3Ghj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram 1st method using Laplace smoohing(Alpha=1)\n",
        "sum_prob = 0\n",
        "bigram_cnt = 0\n",
        "alpha = 1.0\n",
        "#Adding start and end tokens to each of our sentences\n",
        "for sent in x_test:\n",
        "    sent = ['<start>'] + sent + ['<end>']\n",
        "\n",
        "    # Iterate over the bigrams of the sentence\n",
        "    for idx in range(1, len(sent)):\n",
        "      #For every bi-gram combination in our sentence\n",
        "        bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +alpha) / (unigram_counter[(sent[idx-1],)] + alpha*vocabsize) # not including the probabilities of the special start token with word but including the special end token\n",
        "        sum_prob += math.log2(bigram_prob)\n",
        "        bigram_cnt += 1\n",
        "\n",
        "HC = -sum_prob / bigram_cnt\n",
        "perpl = math.pow(2,HC)\n",
        "print(\"Cross Entropy: {0:.3f}\".format(HC)) # calculation of cross entropy and perplexity of our bigram model with Laplace smoothing \n",
        "print(\"perplexity: {0:.3f}\".format(perpl))"
      ],
      "metadata": {
        "id": "w0a30C3Fg5DG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af874a73-5b39-4cf4-c097-cc665d1342bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Entropy: 7.789\n",
            "perplexity: 221.243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prev(w):\n",
        "  count = 0 \n",
        "  for v in vocabtrain:\n",
        "    if bigram_counter[(v,w)]>0:\n",
        "      count += 1\n",
        "  return count"
      ],
      "metadata": {
        "id": "1hysiLJ46ESh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next(w):\n",
        "  count = 0 \n",
        "  for v in vocabtrain:\n",
        "    if bigram_counter[(w,v)]>0:\n",
        "      count += 1\n",
        "  return count"
      ],
      "metadata": {
        "id": "O-H4WLnv6HKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram probability calculation using Kneser Ney smoothing (our implementation)\n",
        "def biprob_w_kneser(w1,w2,tokens=flat_list2,d= 0.5):\n",
        "  S = 0\n",
        "  w1=w1.lower()\n",
        "  w2=w2.lower()\n",
        "  for voc in vocabtrain:\n",
        "    if bigram_counter[(w1,voc)]==0:\n",
        "      S += prev(voc)\n",
        "  if bigram_counter[(w1,w2)]>0:\n",
        "    a= (bigram_counter[(w1,w2)]-d)/(unigram_counter[(w1,)])                             \n",
        "  else:\n",
        "    a= (d/(unigram_counter[(w1,)]))*next(w1)*(prev(w2)/S) \n",
        "  return a"
      ],
      "metadata": {
        "id": "WrJPW0QbiVTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram-Kneser Ney smoothing written to guarantee a smoother running time.\n",
        "(Can be enabled manually)"
      ],
      "metadata": {
        "id": "mr4PrTk5_Mum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "biprob_w_kneser('I', 'am')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1dALskBZvy8",
        "outputId": "b00373a8-5b6d-4c6b-9651-ac8100ffb867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0468586387434555"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "biprob_w_kneser('The', 'Government')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnzgCHX5aSxj",
        "outputId": "c5f0579d-1215-409c-bb2c-c391d48b840d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0012841161363339572"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "biprob_w_kneser('Soviet', 'Union')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgUJoTvAaTBQ",
        "outputId": "bee3e54d-62fe-4f49-8ba5-bf3776e951ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2230769230769231"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4oHLZbqNFXn"
      },
      "outputs": [],
      "source": [
        "# #Bigram 2nd method using using Kneser Ney smoothing \n",
        "# sum_prob = 0\n",
        "# bigram_cnt = 0\n",
        "# alpha = 1.0\n",
        "# for sent in x_test:\n",
        "#     sent = ['<start>'] + sent + ['<end>']\n",
        "\n",
        "#     # Iterate over the bigrmas of the sentence\n",
        "#     for idx in range(1, len(sent)):\n",
        "#         bigram_prob = biprob_w_kneser(sent[idx-1], sent[idx])\n",
        "#         sum_prob += bigram_prob\n",
        "#         bigram_cnt += 1\n",
        "#         #print(idx)\n",
        "\n",
        "# HC = -sum_prob / bigram_cnt\n",
        "# perpl = math.pow(2,HC)\n",
        "# print(\"Cross Entropy: {0:.3f}\".format(HC)) # calculating cross entropy and perplexity of our model using the second model to compare the results \n",
        "# print(\"perplexity: {0:.3f}\".format(perpl)) # with the two different smoothing techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDUI6vh8QYNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b182770f-d9b2-4157-87bd-a40a14c270bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Entropy: 9.686\n",
            "perplexity: 823.750\n"
          ]
        }
      ],
      "source": [
        "#Trigram 1st implementation using Laplace smoothing (therefore a=1.0) \n",
        "sum_prob = 0\n",
        "trigram_cnt = 0\n",
        "\n",
        "for sent in x_test:\n",
        "    sent = ['<start>'] + ['<start>'] + sent + ['<end>'] + ['<end>']\n",
        "\n",
        "    for idx in range(2,len(sent)-1):\n",
        "        trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + alpha*vocabsize)  # not including the probabilities of the special start token with word but including the special end token\n",
        "        sum_prob += math.log2(trigram_prob)\n",
        "        trigram_cnt+=1\n",
        "\n",
        "HC = -sum_prob / trigram_cnt\n",
        "perpl = math.pow(2,HC)\n",
        "print(\"Cross Entropy: {0:.3f}\".format(HC)) # calculation cross entropy and perplexity of the trigram model using Laplace smoothing\n",
        "print(\"perplexity: {0:.3f}\".format(perpl))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3"
      ],
      "metadata": {
        "id": "WkNRdEgy3iZj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkEioZ-Jk6mt"
      },
      "outputs": [],
      "source": [
        " #Function to retrieve key from value of input from given dictionary \n",
        "def get_key(my_dict, val):\n",
        "    for key, value in my_dict.items():\n",
        "         if val == value:\n",
        "             return key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalized sentence correction using beam search for bigrams ( c is the number of possible words/The higher the l the more context aware the model/The higher the r the more we focus on correct spelling)\n",
        "def bi_beam_search_ssj(sente,k = 30, c = 3,l = 0.7, r = 0.3): \n",
        "  sente = sente.lower() #Lowercasing our data\n",
        "  ws = word_tokenize(sente)\n",
        "  ws = ['<start>'] + ws + ['<end>']  # we create sentences for the entire corpus that begin and end with the special tokens (<start>, <end>). We use only one end token because the moment we find the first end token\n",
        "  pw = ['<start>'] \n",
        "  dc = dict()\n",
        "  dc['<start>'] = [0] # dictionary initialization \n",
        "  final = []\n",
        "  for idx in range(len(ws)-2):\n",
        "    dist_temp = []\n",
        "    temp = []\n",
        "    for word in vocabtrain: #  we keep the best k words(Lowest levenstein distance between target word and vocabulary word)\n",
        "      dist_temp.append((ld(word,ws[idx+1]),word))\n",
        "    distance_srt=dist_temp\n",
        "    distance_srt.sort()\n",
        "    distance_k=distance_srt[:k] #Get the K first possible words\n",
        "    for p in pw:\n",
        "      for dk in distance_k:\n",
        "        temp.append((dc[p][0] -l*biprob(p,dk[1])-r*(math.log2(1/(1+ld(dk[1],ws[idx+1])))),(dk[1],p)))\n",
        "    tmp_srt = temp\n",
        "    tmp_srt.sort()\n",
        "    tmp_k= tmp_srt[:c] # it keeps the three best TOTAL probabilities ( not only the three best bigrams of the specific level)\n",
        "    for tk in tmp_k:\n",
        "      dc[tk[1]] = [tk[0]] # the value of added probabilities of the specific sentence so far\n",
        "    pw = [t[1] for t in tmp_k]\n",
        "  ex = dc[pw[0]][0] # starting arbitrary value of ex for the following comparison\n",
        "  for wor in pw:\n",
        "    if dc[wor][0] <= ex: # trying to find the min value ( because we have negative values)\n",
        "      ex = dc[wor][0]\n",
        "      a = dc[wor]\n",
        "  key = get_key(dc, a)\n",
        "  for i in range(idx+1):\n",
        "    final.append(key[0])\n",
        "    key=key[1] # the tuple of the most probable sum of bigrams \n",
        "  final_sent = ' '.join(f for f in final[::-1]) # joins all the words to form the final sentnece \n",
        "  return  final_sent"
      ],
      "metadata": {
        "id": "-s2exF_MjGna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM-ywJ24WHPF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd3e15e5-b977-4438-997f-8618faa18c2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'it is alex'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "bi_beam_search_ssj('I is allxx')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence correction using beamsearch technique on trigrams\n",
        "def tri_beam_search(x,k=30,l = 0.7,r = 0.3): # ( k is the number of words stored in each level/The higher the l the more context aware the model/The higher the r the more we focus on correct spelling)\n",
        "  x=x.lower()  #Lowercasing our data\n",
        "  ws = word_tokenize(x)\n",
        "  ws = ['<start>']+['<start>'] + ws + ['<end>']  # Creating sentences for the entire corpus that begin and end with the special tokens (<start>, <end>). We only use one end token because the moment we find it we know this is the end of the sentence.(No need for a second token)\n",
        "  possible_word = ['<start>']+['<start>']\n",
        "  final= []\n",
        "  for idx in range(len(ws)-3):\n",
        "    temp = [] \n",
        "    distance=[]\n",
        "    for word in vocabtrain: # Storing the best k words(Lowest levenstein distance between target word and vocabulary word)\n",
        "      distance.append((ld(word,ws[idx+2]),word))\n",
        "    distance_srt=distance\n",
        "    distance_srt.sort()\n",
        "    distance_k=distance_srt[:k]\n",
        "    for elem in distance_k: # Using the above k words we calculate the aggregate probability using both levenstein distance and the specific bigrams probability\n",
        "      temp.append((-l*triprob(possible_word[0],possible_word[1],elem[1])-r*(math.log2(1/(1+ld(elem[1],ws[idx+2])))),elem[1]))  \n",
        "    tmp_srt = temp\n",
        "    tmp_srt.sort()\n",
        "    tmp_k= tmp_srt[:1] \n",
        "    a=possible_word[1]\n",
        "    possible_word[0]=a\n",
        "    possible_word[1]= tmp_k[0][1]\n",
        "    final.append(possible_word[1]) #Final sentence prediction\n",
        "    final_pred = ' '.join(f for f in final)\n",
        "  return final_pred"
      ],
      "metadata": {
        "id": "AiYl8ZM8jVv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwklDFC6_1SU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b7bd0072-1142-45b0-8e43-ae4535ba2b71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'after all these years you would like to eat'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "tri_beam_search('Aftre all teese years you wouldd like to meeet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N4n0HVv2dYad"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
